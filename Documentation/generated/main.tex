\documentclass[sigconf]{acmart}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}

% ACM formatting
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}
\acmConference[SIGCSE '25]{Proceedings of the 56th ACM Technical Symposium on Computer Science Education}{March 2025}{Portland, OR, USA}
\acmBooktitle{Proceedings of the 56th ACM Technical Symposium on Computer Science Education (SIGCSE '25), March 2025, Portland, OR, USA}
\acmPrice{15.00}
\acmDOI{10.1145/xxxxxxx.xxxxxxx}
\acmISBN{978-x-xxxx-xxxx-x/xx/xx}

\begin{document}

\title{Confidence-Based Marking in Educational Assessment: Examining the Divergence Between Human Metacognitive Awareness and Artificial Intelligence Overconfidence}

\author{[Your Name]}
\email{[your.email@institution.edu]}
\affiliation{%
  \institution{[Your Institution]}
  \streetaddress{[Street Address]}
  \city{[City]}
  \state{[State]}
  \postcode{[Postcode]}
  \country{[Country]}
}

\begin{abstract}
This research investigates the fundamental differences between human metacognitive processes and artificial intelligence confidence calibration in educational assessment contexts. Testing with confidence-based marking (CBM) was implemented across multiple cohorts and responses were compaired wtih contemporary AI systems. The analysis demonstrates systematic overconfidence in AI responses to multiple-choice questions. We propose that human confidence assessments incorporate multifaceted metacognitive strategies including internal certainty evaluation, contextual alignment consideration, and strategic test-taking behaviors, while AI systems exhibit consistent overconfidence patterns attributable to training data characteristics and lack of metacognitive utility during training. We propose a framework for AI-generated assessment that leverages CBM to identify potentially problematic questions while shifting pedagogical emphasis from content creation to content evaluation, addressing the evolving landscape of generative AI in education.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003456.10003457.10003527.10003531</concept_id>
<concept_desc>Social and professional topics~Computing education</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Social and professional topics~Computing education}
\ccsdesc[300]{Computing methodologies~Machine learning}

\keywords{confidence-based marking, metacognition, artificial intelligence, educational assessment, overconfidence bias}

\maketitle

\section{Introduction}

The assessment of student knowledge has traditionally focused on correctness of responses, yet this approach fails to capture the crucial metacognitive dimension of learning: students' awareness of their own knowledge and uncertainty. Confidence/Certainty based marking (CBM), advocated by researchers such as Gardner-Medwin~\cite{gardner2006confidence}, addresses this limitation by incorporating students' confidence levels into assessment scoring, thereby encouraging metacognitive reflection and providing insights into learning gaps.

We have used CBM over many years both in Norway and New Zealand to improve the quality of feedback to students, improve question quality, and emphasis the importance of meta-cognition. We have a three level confidence system



The emergence of large language models and their integration into educational contexts presents new challenges and opportunities for assessment design. Preliminary observations suggest that artificial intelligence systems exhibit systematically different confidence patterns compared to human learners, with implications for both understanding AI behavior and designing effective assessment strategies in an AI-augmented educational landscape.

This research examines the fundamental differences between human metacognitive processes in confidence assessment and AI confidence calibration, proposing novel approaches to leverage these differences for improved educational outcomes.

\section{The Value of Metacognitive Knowledge in Education}

\subsection{Theoretical Foundations}

Metacognitive knowledge, defined as awareness and understanding of one's own thought processes, represents a critical component of effective learning~\cite{flavell1976metacognitive}. Gardner-Medwin's seminal work on confidence-based marking demonstrates that incorporating confidence assessments into evaluation frameworks provides multiple pedagogical benefits~\cite{gardner2006confidence, gardner2011learning}.

The CBM approach transforms traditional binary correct/incorrect scoring into a multidimensional assessment that rewards accurate self-knowledge while penalizing overconfidence. This methodology encourages students to develop metacognitive awareness by explicitly requiring them to evaluate their certainty levels, thereby promoting more reflective learning approaches.

\subsection{Confidence versus Certainty}

While often used interchangeably in colloquial contexts, confidence and certainty represent distinct psychological constructs in assessment scenarios. Confidence refers to the subjective probability an individual assigns to the correctness of their response, while certainty encompasses the degree of conviction in that assessment. In educational contexts, confidence operates as a measurable construct that can be calibrated against actual performance, providing insights into metacognitive accuracy.

Research in cognitive psychology demonstrates that well-calibrated confidence correlates with improved learning outcomes and knowledge retention~\cite{dunlosky2011four}. Students who accurately assess their confidence levels are better positioned to identify knowledge gaps and allocate study resources effectively.

\subsection{Empirical Evidence}

Our longitudinal data collection across multiple cohorts reveals consistent patterns in human confidence calibration. Students demonstrate varying degrees of metacognitive accuracy, with improvement observed over time as they develop familiarity with CBM frameworks. Initial overconfidence typically gives way to more nuanced self-assessment as students gain experience with the methodology.

[Data presentation section would include specific statistical analyses of your human subject data]

\section{Artificial Intelligence and Systematic Overconfidence}

\subsection{ AI confidence questions}

\subsubsection{ Linear }

\subsubsection { Combined }

\subsubsection { Parallel }

\subsection{Observed Patterns}

Comparative analysis of contemporary AI systems responding to identical multiple-choice questions reveals systematic overconfidence patterns. Unlike human participants who demonstrate variable confidence levels correlated with actual knowledge states, AI systems consistently assign high confidence scores regardless of question difficulty or domain specificity.

Our benchmark evaluation, available at Ludogy.co.nz/calibration\_bench, demonstrates that leading language models exhibit overconfidence rates exceeding 40\% above optimal calibration levels. This systematic bias persists across different model architectures and training methodologies.

\subsection{Theoretical Explanations}

The observed overconfidence in AI systems can be attributed to several fundamental factors:

\textbf{Training Data Characteristics:} The textual corpora used for language model training predominantly contain confident assertions rather than expressions of uncertainty. Academic papers, encyclopedias, and authoritative sources rarely include confidence qualifiers, creating a training environment that reinforces confident response patterns.

\textbf{Lack of Metacognitive Utility:} Unlike human learners who benefit from accurate confidence assessment through improved learning allocation, AI systems derive no direct utility from confidence calibration during inference. The absence of ongoing learning during deployment removes the selective pressure that would otherwise encourage accurate uncertainty estimation.

\textbf{Architectural Limitations:} Current transformer-based architectures lack explicit mechanisms for uncertainty quantification. While attention patterns and token probabilities provide some indication of model confidence, these internal representations are not optimized for accurate confidence reporting.

\subsection{Implications for Test-Time Compute}

The significance of confidence calibration becomes apparent in test-time compute scenarios where AI agents must allocate computational resources dynamically. Accurate uncertainty estimation enables systems to invest additional processing time in areas of genuine uncertainty while expediting confident responses. However, systematic overconfidence undermines this resource allocation strategy, leading to suboptimal performance patterns.

\section{Human Confidence Assessment Mechanisms}

\subsection{Internal Certainty Evaluation}

Human confidence assessment begins with introspective evaluation of knowledge certainty. This process involves accessing declarative and procedural knowledge stores while simultaneously monitoring the fluency and completeness of memory retrieval. Students demonstrate sophisticated awareness of their own cognitive processes, distinguishing between confident knowledge, uncertain recollection, and complete ignorance.

Neurocognitive research suggests that this introspective capability involves prefrontal cortex regions associated with metacognitive monitoring~\cite{fleming2010knowing}. The subjective experience of knowing correlates with measurable neural activity patterns, providing biological foundations for confidence reporting accuracy in educational contexts.

\subsection{Contextual Alignment Consideration}

Human confidence assessment incorporates sophisticated contextual reasoning that extends beyond pure knowledge evaluation. Students consider multiple contextual factors when assessing response confidence:

\textbf{Curricular Emphasis:} Topics receiving extensive coverage in course materials generate higher confidence levels, even when actual knowledge depth may be limited. Students implicitly weight their confidence assessments based on perceived importance within the educational context.

\textbf{Contemporary Relevance:} Current events and recent discussions influence confidence patterns, with students expressing higher certainty for information encountered in multiple contexts or recent time frames.

\textbf{Source Credibility:} The perceived authority of information sources affects confidence levels, with students demonstrating higher certainty for material attributed to respected authorities or repeated across multiple credible sources.

\subsection{Strategic Test-Taking Behavior}

The third component of human confidence assessment involves sophisticated reverse-engineering of assessment intentions. Students engage in strategic reasoning about test design, considering factors such as:

\textbf{Question Construction:} Analysis of distractor plausibility and option formatting provides clues about intended difficulty levels and correct responses.

\textbf{Assessment Context:} The positioning of questions within broader evaluations influences confidence levels, with students adjusting their certainty based on perceived question importance and weighting.

\textbf{Instructor Intentions:} Students attempt to infer instructor priorities and emphasis areas, using this information to calibrate confidence levels for different question types and domains.

These strategic considerations represent uniquely human capabilities that are typically unavailable to AI systems operating without access to curricular context, instructor characteristics, or assessment design principles.

\section{AI-Generated Assessment Framework}

\subsection{Claim-Based Question Generation}

We propose a novel assessment framework that leverages AI capabilities for dynamic question generation while incorporating CBM principles to address quality concerns. In this approach, students submit substantive work representing specific claims or arguments, which then serve as the basis for AI-generated multiple-choice questions.

The process operates as follows:
\begin{enumerate}
\item Students submit original work containing specific claims, arguments, or analyses
\item AI systems generate unique multiple-choice questions targeting comprehension of submitted content
\item Students respond to generated questions using CBM methodology
\item Assessment combines original work quality with demonstrated understanding through question responses
\end{enumerate}

\subsection{Quality Assurance Through Uncertainty}

Recognizing the potential variability in AI-generated question quality, the CBM framework provides a mechanism for students to signal problematic questions through low confidence responses. Students expressing uncertainty about question clarity or appropriateness can indicate this through confidence ratings, enabling instructors to identify potentially flawed assessment items.

This approach transforms potential AI limitations into pedagogical opportunities, teaching students to evaluate question quality while maintaining focus on content comprehension. The negotiated grading process allows for discussion of both content understanding and assessment validity.

\subsection{Pedagogical Implications}

This framework addresses the fundamental challenge posed by generative AI in educational contexts: the shift from content creation to content evaluation as the primary skill. As AI systems become increasingly capable of producing sophisticated written work, educational emphasis must transition toward critical evaluation, synthesis, and quality assessment capabilities.

The claim-based assessment approach requires students to demonstrate deep understanding of submitted work rather than merely producing acceptable content. This shift aligns with evolving workforce requirements where professionals must evaluate AI-generated content rather than compete with AI production capabilities.

\section{Implementation Considerations}

\subsection{Technical Architecture}

Implementation of the proposed framework requires integration of multiple technological components:

\textbf{Content Analysis:} Natural language processing systems must identify key claims and concepts within submitted work, providing the foundation for targeted question generation.

\textbf{Question Generation:} Large language models generate contextually appropriate multiple-choice questions with varying difficulty levels and distractor quality.

\textbf{CBM Integration:} Assessment platforms must capture both response selection and confidence ratings, implementing appropriate scoring algorithms that reward metacognitive accuracy.

\textbf{Quality Monitoring:} Automated systems should flag potentially problematic questions based on student confidence patterns and response distributions.

\subsection{Evaluation Metrics}

The effectiveness of claim-based CBM assessment requires multidimensional evaluation criteria:

\textbf{Content Comprehension:} Traditional accuracy measures for question responses, weighted by confidence levels using established CBM scoring methods.

\textbf{Metacognitive Calibration:} Comparison of stated confidence levels with actual performance, measuring the development of accurate self-assessment capabilities.

\textbf{Question Quality:} Analysis of generated question characteristics, including distractor effectiveness, content validity, and clarity measures.

\textbf{Learning Outcomes:} Longitudinal assessment of student learning gains and retention rates compared to traditional assessment approaches.

\section{Future Directions and Limitations}

\subsection{Research Extensions}

Several research directions emerge from this work:

\textbf{Cross-Domain Validation:} Extension of CBM-AI comparisons across diverse academic disciplines to identify domain-specific patterns in confidence calibration.

\textbf{Longitudinal Studies:} Long-term tracking of student metacognitive development under claim-based assessment frameworks.

\textbf{AI Uncertainty Methods:} Investigation of techniques for improving AI confidence calibration, potentially through specialized training procedures or architectural modifications.

\subsection{Limitations}

This research acknowledges several limitations:

The benchmark evaluation focuses on multiple-choice formats, potentially missing confidence patterns in other assessment modalities. Additionally, the AI systems examined represent a snapshot of current capabilities, with rapid development potentially altering observed patterns.

The proposed framework requires significant technological infrastructure that may not be accessible to all educational institutions. Implementation costs and technical expertise requirements may limit adoption scope.


\section{Discussion of Confidence modifiers}
There are at least eight different modifiers that affect confidence in the answers for human participants:
\begin{enumerate}
    \item Intrinsic Confidence: how confident the student is in their answer based on internal sense of certainty.
    \item Environmental: having seen the question answer pair recently.  This could be in the context of the course, exam preparation, or media coverage of a one of the answers.  This can be affected by there being repeated presentations of information regardless of the persons intrinsic development of confidence, the mere repetition of an question answer pair could increase confidence
    \item Sociocultural factors: Many cultures have conventions related to confidence.  These can be gendered, racial, age related or status factors which will modify the confidence of individuals.
    \item Mental Health: anxiety and depression are likely to reduce confidence indicators, while mania and narcissism is likely to increase the estimate of confidence in the answer.
    \item Question Quality: independent of answering quality there is the potential for the reader to evaluate the question as poor, and so decrease confidence based on the quality of the questions wording.  This ambiguity in the question may make some readers unlikely to use a high confidence.
    \item Accuracy / Objecitve: Some responders will be trying to portray a level of confidence independent of their ability 
    \item Min/Max: Some people will be trying to maximize their score regardless of the accuracy of their confidence, as they are willing to risk being wrong to maximize scores
    \item Reverse Engineering: the reader could increase their confidence in an answer by evaluating the ability, understanding, and motivation of the person asking the question. 
\end{enumerate}
Given these potential influences, the generation of a confidence level in human particpants could be a combination of complext environment, learning, and reasoning processess.  These are unlikely to be similar in the processing by AI.  When comparing students using this confidence based assessment it is important to understand the role of these influences in how different students may be answering these questions.

These pontential influences on confidence are not limited to answering multichoice questions.  Many of them will affect the students ability to work effectively across different cultural contexts.

Being able to have a discussion with a student about why they are "under" or "over" confident based on a numerical assessment of the percentage of questions that should be in each category allows the lecturer to also engage in a discusion of these more complex environmental influences on confidence.

Accurate confidence may lead to better resource allocation of time by the student in the future and more accurate interaction in workplace discussion. When a student can accurately assess their personal intrinsic understanding, they could focus learning activities on understanding the parts of a course or study area where they are weak. Understanding the other features that could be influencing a student could also lead to focusing effort on improving accuracy by addressing other aspect of question answering.
For example a student could improve in each of the areas above by:
\begin{enumerate}
    \item Intrinsic: learning more content knowledge and triangulate information
    \item Environment: pay attention to environmental information, and the wider context of the question setting
    \item Social: Work on understanding the cultural component to there assessment and try to counter the bias in confidence created by their cultural norms.
    \item Mental Health: managing anxiety disorders or depression could have a larger impact on accuracy in confidence than increased study.  Students with impostor syndrome may benefit more from pastoral care than study skills.
    \item Question Quality: The student can give feedback to the question setter about the quality of the questions
    \item Accuracy/Objective:  If the student wants to improve their accuracy then they may need to understand the impact of their other motivations on their answering.  Recognising your range of motivations when answering questions could help the student make conscious choices about their answers.
    \item Min/Max: understanding that the confidence marking is more valuable as feedback than for summative assessment may help students provide accurate assessments of their capability.
    \item Reverse Engineering: Students are often taught to work out what the examiners are wanting as an answer.  This denial of the students own understanding to be compliant to the examination is probably a negative outcome for the assessment system.  It focuses the students not on reality but on making the lecturer/examiner happy.  This has the potential to destroy creativity and make students merely interested in giving the questioner the answer they want. This compliance to doctrine could decrease the value of education and critical thinking.
\end{enumerate}

The role of CBM in developing a deeper understanding of motivations around answering questions and understanding the purpose of knowledge is far more important than merely being able to pull an answer from your head, and respond correctly.  


\section{Conclusion}

The integration of artificial intelligence into educational assessment presents both opportunities and challenges that require careful consideration of fundamental differences between human and artificial cognitive processes. Our research demonstrates that while AI systems exhibit systematic overconfidence patterns attributable to training methodologies and architectural limitations, these characteristics can be leveraged constructively within appropriately designed assessment frameworks.

The proposed claim-based CBM approach addresses the evolving educational landscape where content creation capabilities are increasingly automated, requiring pedagogical emphasis to shift toward content evaluation and quality assessment skills. By combining AI-generated assessment with human metacognitive awareness, educational institutions can maintain assessment validity while preparing students for an AI-augmented professional environment.

The systematic overconfidence observed in AI systems serves as a reminder that technological capabilities must be understood within their limitations and deployed thoughtfully within educational contexts. As generative AI continues to evolve, educational assessment must similarly adapt to leverage artificial intelligence strengths while preserving the uniquely human elements of learning and evaluation.

Future research should continue exploring the intersection of human metacognition and artificial intelligence capabilities, seeking frameworks that enhance rather than replace human cognitive processes in educational contexts. The goal remains the development of learners who can effectively evaluate, synthesize, and build upon both human and artificial intelligence contributions to knowledge creation.

\bibliographystyle{ACM-Reference-Format}
\bibliography{acm-reference}

\end{document}
