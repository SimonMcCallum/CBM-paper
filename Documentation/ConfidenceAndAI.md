# Confidence Marking and AI

tl;dr
1, AIs have inaccurate confidence in their answers
2, Humans need accurate confidence to evaluate and learn
3, Confidence based marking focuses attention on the quality of knowledge not the quantity.
4, Grade humans on their Understanding and confidence, not on quality of knowledge produced in outputs.

## Testing Human Understanding
Traditional assessment methods have focused on a the ability to write an answer as a proxy for understanding. We assume that if you can write a clear description of something you have and understanding of that thing. This is why we ask students to write in their own words.  This is why we keep changing our exam questions so students cannot memorise answers to "fake" understanding. This is why people argue that writing and essay helps you shape and develop your ideas, because the challenge of creating the output both drives and tests understanding.

Most students are taught exam techniques, trained to memorise essay answers, look at previous exams to focus study. These techniques decrease the validity of the exam as a test for understanding, and make it a test of memory. Good assessment design creates assessments which assess the relevant skills and 

## The wrong proxy

The essay has fallen.  Testing a students' ability to submit an essay no longer tests understanding, but the ability to use AI tools, both built into word and running separately. Students do not NEED to think to get unique outputs. Do you want the dishes done, or do you want ME to do the dishes. Most people use a dishwasher, speed of loading and understanding of what should be hand washed is the most important skill (do not put my good knives in the dishwasher).

## Understanding and Meta-knowledge

One of the many failings of Bloom's taxonomy is that is does not differentiate between understanding and metaknowledge.  Knowiedge about your knowledge is critcal in human learning. Our ability to recognise the quality of our knowledege is how we get out of the Dunning-Kruger echo chamber. Traditionally, as you learn you begin to recognise the complexity of any area of knowledge, and you inability to accurately predict how things connect and what they do.  This is the development of meta-knowledge, an understanding of the quality of knowledge  

## Ways to measure Understanding


For years we have known that students have been recieving help with assessments, be it contract cheating, familial support, or AI.  The submitted work is no longer a representation of the students understanding. 


As part of assessment in the 2010s we moved to a submit and interview process.  The goal was that that submission was the claim of understanding and the interview was to assess that understanding. This was practically achieved by having students use change control systems like git for managing development, then submit the final project merely as a tag in the change system, quickly scan the submission, and then interview the students about:
1, why did you include this part of the code. (understanding)
2, how would you change the code. (manipulation)

This can now be automated with AI.  The AI can generate questions based on the submission. This can be in two parts, 1, MCQ focused on testing understanding claimed in the submission; 2, Ask the student which change in the submission would achieve a specific goal - this is an MCQ where the AI suggest 5 different changes to the text and the student needs to read and evaluate which change would be the best to link with the rest of the text (understanding) and make the appropriate change to the text (apply understanding)

## Variants in Assessment and their connection to Metaknowledge


## Testing Understanding

Given the submission is a claim of understanding we need to test the student based on their understanding.

Theory:
Students develop understanding by reading text, editing text, and or writing text. The process of forming meaningful memories and access to information requires effort and time.  Our traditionals proxies for understanding measure the time and effort required to produce and output.  Measuring the output implied the student has spend the time required to change their brain, to initiate traces in the hippocampus and then transfer based on repetition into the neocortex.

We need new ways to test understanding as LLMs have decreased time and effort to create output proxies. These new methods need meet some reqirements: ( requirements for assessment in the age of AI )
1, Tests the students understanding of the area
2, Motivates the student to put time and effort into understanding
3, Tests the quality of thier knowledge, as accurate confidence is critical for learning
4, 


Students are suffering because our focus and our assessments motivate the creation of content rather than the changing of our students brains.  Our traditional education bargan is that students will follow our direction as we understand the journey needed to get to the destination. 

1, Create a MCQ system based on the submitted content from students.  
2, Framing - Lecturer frames what is important about the assignment.  What is the understanding goal? what is the nature and level of the understanding required
3, 

The MCQ collection for a course needs to have some questions which are common to all students, some which are common based on similar claims of knowledge, and some which are unique to the individual student.

We must use a confidence based system to compensate for the variability of the MCQs.  A student can then allocate guessing option to any question that they feel is incorrect/or overly complex.

The process: 
Lecture develops a bank of questions which act as a fram 



Process
1, Attach a webhook to the submission system which triggers when the student submits
2, Student submits work, the webhook is activated and the submitted PDF is sent to the create tool
3, Using Gemini with data security and embedded, or Copilot ( issue is copilot has no API )
4,  

## Testing integration
